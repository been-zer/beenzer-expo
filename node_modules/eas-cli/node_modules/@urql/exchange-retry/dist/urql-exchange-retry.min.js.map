{"version":3,"file":"urql-exchange-retry.min.js","sources":["../src/retryExchange.ts"],"sourcesContent":["import {\n  Source,\n  makeSubject,\n  share,\n  pipe,\n  merge,\n  filter,\n  fromValue,\n  debounce,\n  mergeMap,\n  takeUntil,\n} from 'wonka';\n\nimport {\n  makeOperation,\n  Exchange,\n  Operation,\n  CombinedError,\n  OperationResult,\n} from '@urql/core';\n\nexport interface RetryExchangeOptions {\n  initialDelayMs?: number;\n  maxDelayMs?: number;\n  randomDelay?: boolean;\n  maxNumberAttempts?: number;\n  /** Conditionally determine whether an error should be retried */\n  retryIf?: (error: CombinedError, operation: Operation) => boolean;\n  /** Conditionally update operations as they're retried (retryIf can be replaced with this) */\n  retryWith?: (\n    error: CombinedError,\n    operation: Operation\n  ) => Operation | null | undefined;\n}\n\nexport const retryExchange = ({\n  initialDelayMs,\n  maxDelayMs,\n  randomDelay,\n  maxNumberAttempts,\n  retryIf,\n  retryWith,\n}: RetryExchangeOptions): Exchange => {\n  const MIN_DELAY = initialDelayMs || 1000;\n  const MAX_DELAY = maxDelayMs || 15000;\n  const MAX_ATTEMPTS = maxNumberAttempts || 2;\n  const RANDOM_DELAY = randomDelay !== undefined ? !!randomDelay : true;\n\n  return ({ forward, dispatchDebug }) => ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n    const {\n      source: retry$,\n      next: nextRetryOperation,\n    } = makeSubject<Operation>();\n\n    const retryWithBackoff$ = pipe(\n      retry$,\n      mergeMap((op: Operation) => {\n        const { key, context } = op;\n        const retryCount = (context.retryCount || 0) + 1;\n        let delayAmount = context.retryDelay || MIN_DELAY;\n\n        const backoffFactor = Math.random() + 1.5;\n        // if randomDelay is enabled and it won't exceed the max delay, apply a random\n        // amount to the delay to avoid thundering herd problem\n        if (RANDOM_DELAY && delayAmount * backoffFactor < MAX_DELAY) {\n          delayAmount *= backoffFactor;\n        }\n\n        // We stop the retries if a teardown event for this operation comes in\n        // But if this event comes through regularly we also stop the retries, since it's\n        // basically the query retrying itself, no backoff should be added!\n        const teardown$ = pipe(\n          sharedOps$,\n          filter(op => {\n            return (\n              (op.kind === 'query' || op.kind === 'teardown') && op.key === key\n            );\n          })\n        );\n\n        dispatchDebug({\n          type: 'retryAttempt',\n          message: `The operation has failed and a retry has been triggered (${retryCount} / ${MAX_ATTEMPTS})`,\n          operation: op,\n          data: {\n            retryCount,\n          },\n        });\n\n        // Add new retryDelay and retryCount to operation\n        return pipe(\n          fromValue(\n            makeOperation(op.kind, op, {\n              ...op.context,\n              retryDelay: delayAmount,\n              retryCount,\n            })\n          ),\n          debounce(() => delayAmount),\n          // Stop retry if a teardown comes in\n          takeUntil(teardown$)\n        );\n      })\n    );\n\n    const result$ = pipe(\n      merge([sharedOps$, retryWithBackoff$]),\n      forward,\n      share,\n      filter(res => {\n        // Only retry if the error passes the conditional retryIf function (if passed)\n        // or if the error contains a networkError\n        if (\n          !res.error ||\n          (retryIf\n            ? !retryIf(res.error, res.operation)\n            : !retryWith && !res.error.networkError)\n        ) {\n          return true;\n        }\n\n        const maxNumberAttemptsExceeded =\n          (res.operation.context.retryCount || 0) >= MAX_ATTEMPTS - 1;\n\n        if (!maxNumberAttemptsExceeded) {\n          const operation = retryWith\n            ? retryWith(res.error, res.operation)\n            : res.operation;\n          if (!operation) return true;\n\n          // Send failed responses to be retried by calling next on the retry$ subject\n          // Exclude operations that have been retried more than the specified max\n          nextRetryOperation(operation);\n          return false;\n        }\n\n        dispatchDebug({\n          type: 'retryExhausted',\n          message:\n            'Maximum number of retries has been reached. No further retries will be performed.',\n          operation: res.operation,\n        });\n\n        return true;\n      })\n    ) as Source<OperationResult>;\n\n    return result$;\n  };\n};\n"],"names":["core","require","maxNumberAttempts","retryIf","retryWith","RANDOM_DELAY","initialDelayMs","undefined","source","next","sharedOps$","nextRetryOperation","key","makeSubject","context","delayAmount","teardown$","wonka","filter","dispatchDebug","op","kind","res","share","forward","merge","retryWithBackoff$"],"mappings":"8EAmCAA,EAAAC,QAAA,oCAAA,EAAAC,iBAAAC,aAAAC,cAAAF,oBAQEC,UACAC,gBAEA,MAAAC,EAAAC,GAAAC,4DAIEC,IAAAC,MAAAC,EAAAC,EAAAA,MAAAA,aAQIF,KAAAE,GAAAC,EAAAC,cAAAC,EAAAA,EAAAA,UAAAA,IAAA,MAAAF,cAEAE,iDAKA,QAAAC,KAAAA,SAAA,qBAgBA,MAAAC,EAAAC,EAAAC,QAAAC,IAAA,UAAAC,EAAAC,MAAA,aAAAD,EAAAC,OAAAD,EAAAR,MAAAA,GAAAK,CAEEP,iIAzBFI,aAyDEI,QAAAI,4EAOF,wCAMI,wEASJ,eAaRL,EAAAM,MAAAC,EAAAP,EAAAQ,MAAA,CAAAf,EAAAgB"}